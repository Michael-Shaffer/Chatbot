#!/usr/bin/env python3
from pathlib import Path
from pdf2image import convert_from_path
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import sys
import tempfile

def process_pdf_with_qwen(pdf_path, max_pages=5):
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen-VL-Chat",
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    ).to("cuda" if torch.cuda.is_available() else "cpu").eval()

    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)
    images = convert_from_path(pdf_path, dpi=200)[:max_pages]
    results = []

    for idx, image in enumerate(images):
        with tempfile.NamedTemporaryFile(suffix=".png", delete=True) as temp_img:
            image.save(temp_img.name)
            query = tokenizer.from_list_format([
                {'image': temp_img.name},
                {'text': 'You are a document analyst. Split this document page into logical, self-contained chunks (sections, tables, or paragraphs). Return each chunk with exact formatting preserved. Number each chunk and include headings if present.'}
            ])
            response, _ = model.chat(tokenizer, query=query, history=None)
            results.append({'page': idx + 1, 'text': response})
    return results

if __name__ == "__main__":
    pdf_file = Path(sys.argv[1])
    results = process_pdf_with_qwen(pdf_file)

    for page in results:
        print(f"\n--- Page {page['page']} ---\n{page['text']}")

